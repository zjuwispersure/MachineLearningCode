{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### Necessary Flags ############\n",
    "######################################\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "######## Learning rate flags #############\n",
    "##########################################\n",
    "initial_learning_rate = 0.001\n",
    "learning_rate_decay_factor = 0.95\n",
    "num_epochs_per_decay = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "########## status flags #################\n",
    "#########################################\n",
    "is_training = False\n",
    "fine_tuning = False\n",
    "online_test = True\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:From <ipython-input-5-755dc9c680c7>:17: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From /Users/yanglijuan/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\nWARNING:tensorflow:From /Users/yanglijuan/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nWARNING:tensorflow:From /Users/yanglijuan/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /Users/yanglijuan/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /Users/yanglijuan/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
    }
   ],
   "source": [
    "##########################################\n",
    "####### Load and Organize Data ###########\n",
    "##########################################\n",
    "'''\n",
    "In this part the input must be prepared.\n",
    "\n",
    "   1 - The MNIST data will be downloaded.\n",
    "   2 - The images and labels for both training and testing will be extracted.\n",
    "   3 - The prepared data format(?,784) is different by the appropriate image shape(?,28,28,1) which needs\n",
    "        to be fed to the CNN architecture. So it needs to be reshaped.\n",
    "\n",
    "'''\n",
    "\n",
    "# Download and get MNIST dataset(available in tensorflow.contrib.learn.python.learn.datasets.mnist)\n",
    "# It checks and download MNIST if it's not already downloaded then extract it.\n",
    "# The 'reshape' is True by default to extract feature vectors but we set it to false to we get the original images.\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=True, one_hot=True)\n",
    "train_data = mnist.train.images\n",
    "train_label = mnist.train.labels\n",
    "test_data = mnist.test.images\n",
    "test_label = mnist.test.labels\n",
    "\n",
    "# # The 'input.provide_data' is provided to organize any custom dataset which has specific characteristics.\n",
    "# data = input.provide_data(mnist)\n",
    "\n",
    "# Dimentionality of train\n",
    "dimensionality_train = train_data.shape\n",
    "\n",
    "# Dimensions\n",
    "num_train_samples = dimensionality_train[0]\n",
    "num_features = dimensionality_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch #1, Train Loss=0.524\nTest Accuracy= 0.7727\nEpoch #2, Train Loss=0.515\nTest Accuracy= 0.7792\nEpoch #3, Train Loss=0.511\nTest Accuracy= 0.7841\nEpoch #4, Train Loss=0.510\nTest Accuracy= 0.7859\nEpoch #5, Train Loss=0.508\nTest Accuracy= 0.7852\nEpoch #6, Train Loss=0.506\nTest Accuracy= 0.7830\nEpoch #7, Train Loss=0.505\nTest Accuracy= 0.7821\nEpoch #8, Train Loss=0.504\nTest Accuracy= 0.7890\nEpoch #9, Train Loss=0.504\nTest Accuracy= 0.7886\nEpoch #10, Train Loss=0.504\nTest Accuracy= 0.7879\nFinal Test Accuracy is 0.79\n"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    decay_steps = int(num_train_samples / batch_size * num_epochs_per_decay)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, learning_rate_decay_factor, staircase=True, name=\"exponential_decay_learning_rate\")\n",
    "\n",
    "    #######\n",
    "    ### Defining place holders\n",
    "    #######\n",
    "\n",
    "    image_place = tf.placeholder(tf.float32, shape=([None, num_features]), name='image')\n",
    "    label_place = tf.placeholder(tf.float32, shape=([None, num_classes]), name='gt')\n",
    "    dropout_param = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "    #####\n",
    "    ### MODEL(MPL with two hidden layer)\n",
    "    ####\n",
    "\n",
    "    net = tf.contrib.layers.fully_connected(inputs=image_place, num_outputs=250, scope='fc-1')\n",
    "    net = tf.contrib.layers.fully_connected(inputs=net, num_outputs=250, scope='fc-2')\n",
    "    logits_pre_softmax = tf.contrib.layers.fully_connected(inputs=net, num_outputs=num_classes, scope='fc-3')\n",
    "    softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_pre_softmax, labels=label_place))\n",
    "    #tf.argmax返回的是vector中的最大值的索引号，如果vector是一个向量，那就返回一个值，如果是一个矩阵，那就返回一个向量；axis=1是行为单位，返回最大列的索引，这里就是判断是否一致。\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_pre_softmax,1), tf.argmax(label_place,1)), tf.float32))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    with tf.name_scope('train_scope'):\n",
    "        #grads = optimizer.compute_gradients(softmax_loss)\n",
    "        #train_op = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "        train_op = optimizer.minimize(softmax_loss, global_step=global_step)\n",
    "    \n",
    "    # Summaries for loss and accuracy\n",
    "    tf.summary.scalar(\"loss\", softmax_loss, collections=['train', 'test'])\n",
    "    tf.summary.scalar(\"accuracy\", accuracy, collections=['train', 'test'])\n",
    "    tf.summary.scalar(\"global_step\", global_step, collections=['train'])\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate, collections=['train'])\n",
    "    \n",
    "    # Merge all summaries together.\n",
    "    summary_train_op = tf.summary.merge_all('train')\n",
    "    summary_test_op = tf.summary.merge_all('test')\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=allow_soft_placement, log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(graph=graph, config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(num_epochs):\n",
    "            total_batch_training = int(train_data.shape[0] / batch_size)\n",
    "\n",
    "            for batch_num in range(total_batch_training):\n",
    "                start_idx = batch_num * batch_size\n",
    "                end_idx = (batch_num + 1) * batch_size\n",
    "\n",
    "                train_batch_data, train_batch_label = train_data[start_idx:end_idx], train_label[start_idx:end_idx]\n",
    "\n",
    "                batch_loss,_,training_step = sess.run(\n",
    "                    [softmax_loss, train_op, global_step], \n",
    "                    feed_dict={image_place:train_batch_data, \n",
    "                        label_place:train_batch_label, \n",
    "                        dropout_param:0.5})\n",
    "\n",
    "            print(\"Epoch #\" + str(epoch+1) + \", Train Loss=\" + \"{:.3f}\".format(batch_loss))\n",
    "\n",
    "            if online_test:\n",
    "                # WARNING: In this evaluation the whole test data is fed. In case the test data is huge this implementation\n",
    "                #          may lead to memory error. In presense of large testing samples, batch evaluation on testing is\n",
    "                #          recommended as in the training phase.\n",
    "                test_accuracy_epoch, test_summaries = sess.run(\n",
    "                    [accuracy, summary_test_op],\n",
    "                    feed_dict={image_place: test_data,\n",
    "                               label_place: test_label,\n",
    "                               dropout_param: 1.})\n",
    "                print(\"Test Accuracy= \" + \\\n",
    "                      \"{:.4f}\".format(test_accuracy_epoch))\n",
    "\n",
    "                ###########################################################\n",
    "                ########## Write the summaries for test phase #############\n",
    "                ###########################################################\n",
    "\n",
    "                # Returning the value of global_step if necessary\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "\n",
    "        # Evaluation of the model\n",
    "        total_test_accuracy = sess.run(accuracy, feed_dict={\n",
    "            image_place: test_data,\n",
    "            label_place: test_label,\n",
    "            dropout_param: 1.})\n",
    "\n",
    "        print(\"Final Test Accuracy is %.2f\" % total_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}